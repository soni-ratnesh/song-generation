{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Generation\n",
    "<br>\n",
    "The Neural Network will generate a new ,\"fake\" song script, based on patterns it recognizes in the training data. It can be used to generate new song lyrics.\n",
    "\n",
    "## Introduction \n",
    "We will implement character level RNN to make our own song lyrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using [55000+ Song Lyrics](https://www.kaggle.com/mousehead/songlyrics/kernels) dataset for training our model. the dataset contains song lyrics of different authers. For the initial steps we will be<br>\n",
    " > 1. load in this data and look at some samples.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required lib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('data/songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only need `text` for training we will be extracting text and discard else for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Look at her face, it's a wonderful face  \\nAnd...\n",
       "1    Take it easy with me, please  \\nTouch me gentl...\n",
       "2    I'll never know why I had to go  \\nWhy I had t...\n",
       "3    Making somebody happy is a question of give an...\n",
       "4    Making somebody happy is a question of give an...\n",
       "5    Well, you hoot and you holler and you make me ...\n",
       "6    Down in the street they're all singing and sho...\n",
       "7    Chiquitita, tell me what's wrong  \\nYou're enc...\n",
       "8    I was out with the morning sun  \\nCouldn't sle...\n",
       "9    I'm waitin' for you baby  \\nI'm sitting all al...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract song lyrics\n",
    "text = data[\"text\"]\n",
    "#print text sample\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Now to understand song and have sence of dat and its structure we will be exploring dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Number of unique words(approx): 210321\n",
      "Number of lines: 57650\n",
      "Average number of words in each line: 42.11129228100607\n",
      "\n",
      "The lines 0 to 10:\n",
      "Look at her face, it's a wonderful face  \n",
      "And it means something special to me  \n",
      "Look at the way that she smiles when she sees me  \n",
      "How lucky can one fellow be?  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, without her I'm blue  \n",
      "And if she ever leaves me what could I do, what could I do?  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "corpus = [sent for sent in text]\n",
    "print('Dataset Stats')\n",
    "print('Number of unique words(approx): {}'.format(len({word: None for sent in corpus for word in sent.split()})))\n",
    "\n",
    "\n",
    "print('Number of lines: {}'.format(len(corpus)))\n",
    "\n",
    "word_count_line = [len(song.split('\\n')) for song in corpus]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "lines = [line for song in corpus for line in song.split('\\n')]\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(0,10))\n",
    "print('\\n'.join(lines[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "In this section we will process and clean data for better undeerstanding. we will be implement the following pre-processing functions below:\n",
    "\n",
    "    1. Lookup Table\n",
    "    2. Tokenize Punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "\n",
    "For word embedding we have created two `dict` for the following purposes,<br>\n",
    "\n",
    "    1. For convertion of word to integer\n",
    "    2. Getting word to corresponding word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    unique_words = tuple(set(text))\n",
    "    int_word = dict(enumerate(unique_words))\n",
    "    word_int = {int_word[i]: i for i in int_word}\n",
    "    \n",
    "    # return tuple\n",
    "    return (word_int, int_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation\n",
    "As for text generation punctuation plays a vital role, we will be tokenzing it. we will split songs with `'\\n'`. The words like `baby` and `baby!` will be diffent in previous case. We will replace the puctuation with some word that is very most likely to come in our sample.<br>\n",
    "<br>\n",
    "For this purpose we will create a `dict` that maps puntuation to its new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"    \n",
    "    punct_dict = {'.': \"||Period||\",\n",
    "                  ',': \"||Comma||\",\n",
    "                  '\"': \"||QuotationMark||\",\n",
    "                  ';': \"||Semicolon||\",\n",
    "                  '!': \"||ExclamationMark||\",\n",
    "                  '?': \"||QuestionMark||\",\n",
    "                  '(': \"||LeftParentheses||\",\n",
    "                  ')': \"||RightParentheses||\",\n",
    "                  '-': \"||Dash||\",\n",
    "                  '\\n':\"||Return||\"}\n",
    "\n",
    "    \n",
    "        \n",
    "    return punct_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving current progress\n",
    "As the project can large, we will be saving current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "text = \"\\n\".join(line for line in lines)\n",
    "\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "    \n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('checkpoint/preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "In this section, we'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    words = np.array(words)\n",
    "    batch_len = batch_size*sequence_length\n",
    "    n_batches = len(words)//batch_len\n",
    "    \n",
    "    words = words[:batch_len*n_batches]\n",
    "    \n",
    "    feature, target = [], []\n",
    "    \n",
    "    for ii in range(0, len(words), sequence_length):\n",
    "        x = words[ii:ii+sequence_length]\n",
    "        feature.append(x)\n",
    "        try:\n",
    "            y = words[ii+sequence_length]\n",
    "        except:\n",
    "            y = x[0]\n",
    "        target.append(y)\n",
    "        \n",
    "\n",
    "    feature, target = np.asarray(feature), np.asarray(target)\n",
    "    feature, target = torch.from_numpy(feature), torch.from_numpy(target)\n",
    "    dataset = TensorDataset(feature, target)\n",
    "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample X: torch.Size([10, 5])\n",
      "Sample X:\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44],\n",
      "        [45, 46, 47, 48, 49]], dtype=torch.int32)\n",
      "\n",
      "Shape of sample Y: torch.Size([10])\n",
      "Sample Y:\n",
      "tensor([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 45], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = np.arange(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = next(data_iter)\n",
    "\n",
    "\n",
    "print(f\"Shape of sample X: {sample_x.shape}\")\n",
    "print(\"Sample X:\")\n",
    "print(sample_x)\n",
    "print()\n",
    "print(f\"Shape of sample Y: {sample_y.shape}\")\n",
    "print(\"Sample Y:\")\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU laye\n",
    "    rs\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # set class variables\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        embedded = self.embedding(nn_input)\n",
    "#         print(embedded.size)\n",
    "        rnn_out, hidden = self.lstm(embedded, hidden)\n",
    "#         print(rnn_out.shape)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        rnn_out = rnn_out.contiguous().view((-1, self.hidden_dim))\n",
    "        out = self.fc(rnn_out)\n",
    "        out = out.view(nn_input.shape[0],-1, self.output_size)\n",
    "        out = out[:,-1]\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            new_weights = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            new_weights = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_())\n",
    "        return new_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propogation of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "  \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "        \n",
    "    # perform backpropagation and optimization\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "   \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output, hidden = rnn.forward(inp, hidden)\n",
    "    \n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    \"\"\"\n",
    "    Trains the given RNN model for given number of epochs\n",
    "    :param rnn: The PyTorch Module that holds the neural network\n",
    "    :param optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param n_epochs: number of epochs\n",
    "    :param show_every_n_batches: after how many epoch loss to show\n",
    "    :return: trained rnn model\n",
    "    \"\"\"\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "\n",
    "1. Set `sequence_length` to the length of a sequence.\n",
    "2. Set `batch_size` to the batch size.\n",
    "3. Set `num_epochs` to the number of epochs to train for.\n",
    "4. Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "5. Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "6. Set `output_size` to the desired size of the output.\n",
    "7. Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "8. Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "9. Set `n_layers` to the number of layers/cells in your RNN.\n",
    "10. Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "\n",
    "sequence_length =32   \n",
    "batch_size = 128\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 512\n",
    "n_layers =  2\n",
    "\n",
    "\n",
    "show_every_n_batches = 100\n",
    "\n",
    "save_path = './save/trained_rnn.pt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "\n",
    "torch.save(trained_rnn, save_filename)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint II\n",
    "After training the model is saved in save dir. Notebook can be resumed from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
