{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Generation\n",
    "<br>\n",
    "The Neural Network will generate a new ,\"fake\" song script, based on patterns it recognizes in the training data. It can be used to generate new song lyrics.\n",
    "\n",
    "## Introduction \n",
    "We will implement character level RNN to make our own song lyrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using [55000+ Song Lyrics](https://www.kaggle.com/mousehead/songlyrics/kernels) dataset for training our model. the dataset contains song lyrics of different authers. For the initial steps we will be<br>\n",
    " > 1. load in this data and look at some samples.\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required lib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('data/songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only need `text` for training we will be extracting text and discard else for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Look at her face, it's a wonderful face  \\nAnd...\n",
       "1    Take it easy with me, please  \\nTouch me gentl...\n",
       "2    I'll never know why I had to go  \\nWhy I had t...\n",
       "3    Making somebody happy is a question of give an...\n",
       "4    Making somebody happy is a question of give an...\n",
       "5    Well, you hoot and you holler and you make me ...\n",
       "6    Down in the street they're all singing and sho...\n",
       "7    Chiquitita, tell me what's wrong  \\nYou're enc...\n",
       "8    I was out with the morning sun  \\nCouldn't sle...\n",
       "9    I'm waitin' for you baby  \\nI'm sitting all al...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract song lyrics\n",
    "text = data[\"text\"]\n",
    "#print text sample\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Now to understand song and have sence of dat and its structure we will be exploring dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Number of unique words(approx): 210321\n",
      "Number of lines: 57650\n",
      "Average number of words in each line: 42.11129228100607\n",
      "\n",
      "The lines 0 to 10:\n",
      "Look at her face, it's a wonderful face  \n",
      "And it means something special to me  \n",
      "Look at the way that she smiles when she sees me  \n",
      "How lucky can one fellow be?  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, without her I'm blue  \n",
      "And if she ever leaves me what could I do, what could I do?  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "corpus = [sent for sent in text]\n",
    "print('Dataset Stats')\n",
    "print('Number of unique words(approx): {}'.format(len({word: None for sent in corpus for word in sent.split()})))\n",
    "\n",
    "\n",
    "print('Number of lines: {}'.format(len(corpus)))\n",
    "\n",
    "word_count_line = [len(song.split('\\n')) for song in corpus]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "lines = [line for song in corpus for line in song.split('\\n')]\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(0,10))\n",
    "print('\\n'.join(lines[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "In this section we will process and clean data for better undeerstanding. we will be implement the following pre-processing functions below:\n",
    "\n",
    "    1. Lookup Table\n",
    "    2. Tokenize Punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "\n",
    "For word embedding we have created two `dict` for the following purposes,<br>\n",
    "\n",
    "    1. For convertion of word to integer\n",
    "    2. Getting word to corresponding word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    unique_words = tuple(set(text))\n",
    "    int_word = dict(enumerate(unique_words))\n",
    "    word_int = {int_word[i]: i for i in int_word}\n",
    "    \n",
    "    # return tuple\n",
    "    return (word_int, int_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation\n",
    "As for text generation punctuation plays a vital role, we will be tokenzing it. we will split songs with `'\\n'`. The words like `baby` and `baby!` will be diffent in previous case. We will replace the puctuation with some word that is very most likely to come in our sample.<br>\n",
    "<br>\n",
    "For this purpose we will create a `dict` that maps puntuation to its new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"    \n",
    "    punct_dict = {'.': \"||Period||\",\n",
    "                  ',': \"||Comma||\",\n",
    "                  '\"': \"||QuotationMark||\",\n",
    "                  ';': \"||Semicolon||\",\n",
    "                  '!': \"||ExclamationMark||\",\n",
    "                  '?': \"||QuestionMark||\",\n",
    "                  '(': \"||LeftParentheses||\",\n",
    "                  ')': \"||RightParentheses||\",\n",
    "                  '-': \"||Dash||\",\n",
    "                  '\\n':\"||Return||\"}\n",
    "\n",
    "    \n",
    "        \n",
    "    return punct_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving current progress\n",
    "As the project can large, we will be saving current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "text = \"\\n\".join(line for line in lines)\n",
    "\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "    \n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('checkpoint/preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "In this section, we'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    words = np.array(words)\n",
    "    batch_len = batch_size*sequence_length\n",
    "    n_batches = len(words)//batch_len\n",
    "    words = words[:batch_len*n_batches]\n",
    "    feature, target = [], []\n",
    "    for ii in range(0, len(words), sequence_length):\n",
    "        x = words[ii:ii+sequence_length]\n",
    "        feature.append(x)\n",
    "        try:\n",
    "            y = words[ii+sequence_length]\n",
    "        except:\n",
    "            y = x[0]\n",
    "        target.append(y)\n",
    "#     print(feature, target)\n",
    "    feature, target = np.asarray(feature), np.asarray(target)\n",
    "    feature, target = torch.from_numpy(feature), torch.from_numpy(target)\n",
    "    dataset = TensorDataset(feature, target)\n",
    "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44],\n",
      "        [45, 46, 47, 48, 49]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 45], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = np.arange(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = next(data_iter)\n",
    "\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
